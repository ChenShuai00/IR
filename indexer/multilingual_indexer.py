import json
from collections import defaultdict
from pathlib import Path
from tqdm import tqdm  # è¿›åº¦æ¡åº“
from config.settings import INDEX_CONFIG
from utils.tokenizer import tokenize
from utils.helpers import load_stopwords


class MultilingualIndexer:
    def __init__(self):
        self.index_dir = Path(INDEX_CONFIG['index_dir'])
        self.documents_path = Path(INDEX_CONFIG['documents_path'])
        self.stopwords = {
            lang: load_stopwords(path)
            for lang, path in INDEX_CONFIG['stopwords'].items()
        }

        # æ•°æ®ç»“æ„
        self.inverted_index = defaultdict(dict)
        self.documents = {}
        self.doc_lengths = {}
        self.avg_doc_length = 0

        # åˆ›å»ºç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
        self.index_dir.mkdir(parents=True, exist_ok=True)
        self.documents_path.parent.mkdir(parents=True, exist_ok=True)

    def process_document(self, doc_id, document):
        """å¤„ç†å•ä¸ªæ–‡æ¡£å¹¶æ›´æ–°ç´¢å¼•"""
        try:
            # é˜²å¾¡æ€§æ£€æŸ¥
            if not all(k in document for k in ['title', 'content', 'url']):
                raise ValueError("æ–‡æ¡£ç¼ºå°‘å¿…è¦å­—æ®µ")

            # è‡ªåŠ¨æ£€æµ‹è¯­è¨€
            text = f"{document['title']} {document['content']}"
            language = 'zh' if any('\u4e00' <= c <= '\u9fff' for c in text) else 'en'

            # åˆ†è¯å¤„ç†
            tokens = tokenize(text, language)
            if not tokens:
                return 0  # è·³è¿‡ç©ºå†…å®¹æ–‡æ¡£

            # è¯é¢‘ç»Ÿè®¡ï¼ˆè¿‡æ»¤åœç”¨è¯å’ŒçŸ­è¯ï¼‰
            term_freq = defaultdict(int)
            for token in tokens:
                if len(token) > 1 and token not in self.stopwords[language]:
                    term_freq[token] += 1

            # æ›´æ–°å€’æ’ç´¢å¼•
            for term, tf in term_freq.items():
                self.inverted_index[term][doc_id] = tf

            # å­˜å‚¨æ–‡æ¡£å…ƒæ•°æ®
            self.documents[doc_id] = {
                'url': document['url'],
                'title': document['title'],
                'content': document['content'],
                'language': language,
                'length': len(tokens)
            }

            return len(tokens)

        except Exception as e:
            print(f"\nå¤„ç†æ–‡æ¡£ {doc_id} æ—¶å‡ºé”™: {str(e)}")
            return 0

    def build_from_raw_data(self, raw_data_dir):
        """ä»åŸå§‹æ•°æ®æ„å»ºç´¢å¼•"""
        files = list(Path(raw_data_dir).glob('*.json'))
        if not files:
            raise FileNotFoundError(f"æœªæ‰¾åˆ°JSONæ–‡ä»¶äº: {raw_data_dir}")

        total_length = 0

        # ä½¿ç”¨tqdmè¿›åº¦æ¡
        with tqdm(files, desc="ğŸ”„ æ„å»ºç´¢å¼•", unit="doc",
                  bar_format="{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]") as pbar:
            for doc_id, filepath in enumerate(pbar):
                try:
                    with open(filepath, 'r', encoding='utf-8') as f:
                        document = json.load(f)

                    length = self.process_document(doc_id, document)
                    total_length += length

                    # åŠ¨æ€æ›´æ–°è¿›åº¦æ¡ä¿¡æ¯
                    pbar.set_postfix({
                        'è¯é¡¹': len(self.inverted_index),
                        'avg_len': f"{total_length / (doc_id + 1):.1f}" if doc_id > 0 else '0'
                    })

                except json.JSONDecodeError:
                    pbar.write(f"âš ï¸ æ–‡ä»¶è§£æå¤±è´¥: {filepath.name}")
                except Exception as e:
                    pbar.write(f"âš ï¸ å¤„ç† {filepath.name} æ—¶å‡ºé”™: {str(e)}")

        # è®¡ç®—å¹³å‡æ–‡æ¡£é•¿åº¦
        if len(self.documents) > 0:
            self.avg_doc_length = total_length / len(self.documents)

        self.save_index()

    def save_index(self):
        """ä¿å­˜ç´¢å¼•åˆ°æ–‡ä»¶"""
        # ä¿å­˜å€’æ’ç´¢å¼•
        with open(self.index_dir / 'inverted_index.json', 'w', encoding='utf-8') as f:
            json.dump(self.inverted_index, f, ensure_ascii=False, indent=2)

        # ä¿å­˜å…ƒæ•°æ®
        with open(self.index_dir / 'meta.json', 'w', encoding='utf-8') as f:
            json.dump({
                'doc_lengths': {doc_id: doc['length']
                                for doc_id, doc in self.documents.items()},
                'avg_doc_length': self.avg_doc_length,
                'total_docs': len(self.documents)
            }, f, ensure_ascii=False, indent=2)

        # ä¿å­˜æ–‡æ¡£æ•°æ®
        with open(self.documents_path, 'w', encoding='utf-8') as f:
            json.dump(self.documents, f, ensure_ascii=False, indent=2)

        print(f"\nâœ… ç´¢å¼•æ„å»ºå®Œæˆï¼ä¿å­˜åˆ°: {self.index_dir}")


if __name__ == "__main__":
    try:
        indexer = MultilingualIndexer()
        indexer.build_from_raw_data('../data/raw_clean')  # ä¿®æ”¹ä¸ºæ‚¨çš„å®é™…è·¯å¾„
    except Exception as e:
        print(f"âŒ ç´¢å¼•æ„å»ºå¤±è´¥: {str(e)}")